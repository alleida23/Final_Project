{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2939a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f6e669f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>series</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>description</th>\n",
       "      <th>language</th>\n",
       "      <th>isbn</th>\n",
       "      <th>genres</th>\n",
       "      <th>characters</th>\n",
       "      <th>bookFormat</th>\n",
       "      <th>...</th>\n",
       "      <th>publishDate</th>\n",
       "      <th>firstPublishDate</th>\n",
       "      <th>awards</th>\n",
       "      <th>numRatings</th>\n",
       "      <th>ratingsByStars</th>\n",
       "      <th>likedPercent</th>\n",
       "      <th>setting</th>\n",
       "      <th>bbeScore</th>\n",
       "      <th>bbeVotes</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>The Hunger Games #1</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>4.33</td>\n",
       "      <td>WINNING MEANS FAME AND FORTUNE.LOSING MEANS CE...</td>\n",
       "      <td>English</td>\n",
       "      <td>9.78044E+12</td>\n",
       "      <td>['Young Adult', 'Fiction', 'Dystopia', 'Fantas...</td>\n",
       "      <td>['Katniss Everdeen', 'Peeta Mellark', 'Cato (H...</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>09/14/08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Locus Award Nominee for Best Young Adult Boo...</td>\n",
       "      <td>6376780</td>\n",
       "      <td>['3444695', '1921313', '745221', '171994', '93...</td>\n",
       "      <td>96.0</td>\n",
       "      <td>['District 12, Panem', 'Capitol, Panem', 'Pane...</td>\n",
       "      <td>2993816</td>\n",
       "      <td>30516</td>\n",
       "      <td>5.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Harry Potter #5</td>\n",
       "      <td>J.K. Rowling, Mary GrandPré (Illustrator)</td>\n",
       "      <td>4.50</td>\n",
       "      <td>There is a door at the end of a silent corrido...</td>\n",
       "      <td>English</td>\n",
       "      <td>9.78044E+12</td>\n",
       "      <td>['Fantasy', 'Young Adult', 'Fiction', 'Magic',...</td>\n",
       "      <td>['Sirius Black', 'Draco Malfoy', 'Ron Weasley'...</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>...</td>\n",
       "      <td>09/28/04</td>\n",
       "      <td>06/21/03</td>\n",
       "      <td>['Bram Stoker Award for Works for Young Reader...</td>\n",
       "      <td>2507623</td>\n",
       "      <td>['1593642', '637516', '222366', '39573', '14526']</td>\n",
       "      <td>98.0</td>\n",
       "      <td>['Hogwarts School of Witchcraft and Wizardry (...</td>\n",
       "      <td>2632233</td>\n",
       "      <td>26923</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>4.28</td>\n",
       "      <td>The unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>English</td>\n",
       "      <td>1E+13</td>\n",
       "      <td>['Classics', 'Fiction', 'Historical Fiction', ...</td>\n",
       "      <td>['Scout Finch', 'Atticus Finch', 'Jem Finch', ...</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>...</td>\n",
       "      <td>05/23/06</td>\n",
       "      <td>07-11-1960</td>\n",
       "      <td>['Pulitzer Prize for Fiction (1961)', 'Audie A...</td>\n",
       "      <td>4501075</td>\n",
       "      <td>['2363896', '1333153', '573280', '149952', '80...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>['Maycomb, Alabama (United States)']</td>\n",
       "      <td>2269402</td>\n",
       "      <td>23328</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title                 series  \\\n",
       "0                           The Hunger Games    The Hunger Games #1   \n",
       "1  Harry Potter and the Order of the Phoenix        Harry Potter #5   \n",
       "2                      To Kill a Mockingbird  To Kill a Mockingbird   \n",
       "\n",
       "                                      author  rating  \\\n",
       "0                            Suzanne Collins    4.33   \n",
       "1  J.K. Rowling, Mary GrandPré (Illustrator)    4.50   \n",
       "2                                 Harper Lee    4.28   \n",
       "\n",
       "                                         description language         isbn  \\\n",
       "0  WINNING MEANS FAME AND FORTUNE.LOSING MEANS CE...  English  9.78044E+12   \n",
       "1  There is a door at the end of a silent corrido...  English  9.78044E+12   \n",
       "2  The unforgettable novel of a childhood in a sl...  English        1E+13   \n",
       "\n",
       "                                              genres  \\\n",
       "0  ['Young Adult', 'Fiction', 'Dystopia', 'Fantas...   \n",
       "1  ['Fantasy', 'Young Adult', 'Fiction', 'Magic',...   \n",
       "2  ['Classics', 'Fiction', 'Historical Fiction', ...   \n",
       "\n",
       "                                          characters bookFormat  ...  \\\n",
       "0  ['Katniss Everdeen', 'Peeta Mellark', 'Cato (H...  Hardcover  ...   \n",
       "1  ['Sirius Black', 'Draco Malfoy', 'Ron Weasley'...  Paperback  ...   \n",
       "2  ['Scout Finch', 'Atticus Finch', 'Jem Finch', ...  Paperback  ...   \n",
       "\n",
       "  publishDate firstPublishDate  \\\n",
       "0    09/14/08              NaN   \n",
       "1    09/28/04         06/21/03   \n",
       "2    05/23/06       07-11-1960   \n",
       "\n",
       "                                              awards numRatings  \\\n",
       "0  ['Locus Award Nominee for Best Young Adult Boo...    6376780   \n",
       "1  ['Bram Stoker Award for Works for Young Reader...    2507623   \n",
       "2  ['Pulitzer Prize for Fiction (1961)', 'Audie A...    4501075   \n",
       "\n",
       "                                      ratingsByStars likedPercent  \\\n",
       "0  ['3444695', '1921313', '745221', '171994', '93...         96.0   \n",
       "1  ['1593642', '637516', '222366', '39573', '14526']         98.0   \n",
       "2  ['2363896', '1333153', '573280', '149952', '80...         95.0   \n",
       "\n",
       "                                             setting bbeScore  bbeVotes price  \n",
       "0  ['District 12, Panem', 'Capitol, Panem', 'Pane...  2993816     30516  5.09  \n",
       "1  ['Hogwarts School of Witchcraft and Wizardry (...  2632233     26923  7.38  \n",
       "2               ['Maycomb, Alabama (United States)']  2269402     23328   NaN  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(52478, 23)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gr_data = pd.read_csv('./1. Original_df/GoodReads.Best_Books_Ever 2.csv')\n",
    "\n",
    "display(gr_data.head(3), gr_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8f3e9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISBN: None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a request to the page URL\n",
    "page = requests.get('https://www.google.com/search?q=get+isbn+Pleasant+Day&sxsrf=APwXEddIVweOZXdkGXz-y50v-0EtlEO30g%3A1681152996584&ei=5Fs0ZJacI6imkdUPmpO9sA8&ved=0ahUKEwiWnrL5_p_-AhUoU6QEHZpJD_YQ4dUDCA8&uact=5&oq=get+isbn+Pleasant+Day&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCAAQogQyBQgAEKIEOgoIABBHENYEELADSgQIQRgAUPoYWPoYYI4caAJwAXgAgAFZiAFZkgEBMZgBAKABAcgBCMABAQ&sclient=gws-wiz-serp')\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Find isbn\n",
    "isbn = soup.select_one('#_71s0ZISJKtSmkdUP15-liAo_17 > div.mR2gOd > div > div > a:nth-child(1) > div > div > div > div.bVj5Zb.FozYP')\n",
    "\n",
    "# Print the result\n",
    "print('ISBN:', isbn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e280ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85bd6aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages: 260 pages, ebook\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a request to the page URL\n",
    "page = requests.get('https://www.goodreads.com/book/show/30346601-brainwalker?from_search=true&from_srp=true&qid=5YLuxvim2y&rank=1')\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Find the number of pages\n",
    "pages = soup.select_one('#__next > div > main > div.BookPage__gridContainer > div.BookPage__rightColumn > div.BookPage__mainContent > div.BookPageMetadataSection > div.BookDetails > div > span:nth-child(1) > span > div > p:nth-child(1)').text.strip()\n",
    "\n",
    "# Print the result\n",
    "print('Pages:', pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac9f938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50d05a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf3bff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a220b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# extract author name and collaborators\n",
    "gr_data['author'] = gr_data['author'].astype(str)\n",
    "gr_data['other_collabs'] = gr_data['author'].apply(lambda x: ', '.join(re.split(',|\\(|:', x)[1:]).strip())\n",
    "gr_data['author'] = gr_data['author'].apply(lambda x: re.split(',|\\(|:', x)[0].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84827018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publishDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>09/14/08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>09/28/04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>05/23/06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>10-10-2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twilight</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "      <td>09-06-2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52473</th>\n",
       "      <td>Fractured</td>\n",
       "      <td>Cheri Schmidt</td>\n",
       "      <td>May 28th 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52474</th>\n",
       "      <td>Anasazi</td>\n",
       "      <td>Emma Michaels</td>\n",
       "      <td>August 5th 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52475</th>\n",
       "      <td>Marked</td>\n",
       "      <td>Kim Richardson</td>\n",
       "      <td>March 18th 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52476</th>\n",
       "      <td>Wayward Son</td>\n",
       "      <td>Tom Pollack</td>\n",
       "      <td>September 1st 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52477</th>\n",
       "      <td>Daughter of Helaman</td>\n",
       "      <td>Misty Moncur</td>\n",
       "      <td>May 8th 2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52478 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title           author  \\\n",
       "0                               The Hunger Games  Suzanne Collins   \n",
       "1      Harry Potter and the Order of the Phoenix     J.K. Rowling   \n",
       "2                          To Kill a Mockingbird       Harper Lee   \n",
       "3                            Pride and Prejudice      Jane Austen   \n",
       "4                                       Twilight  Stephenie Meyer   \n",
       "...                                          ...              ...   \n",
       "52473                                  Fractured    Cheri Schmidt   \n",
       "52474                                    Anasazi    Emma Michaels   \n",
       "52475                                     Marked   Kim Richardson   \n",
       "52476                                Wayward Son      Tom Pollack   \n",
       "52477                        Daughter of Helaman     Misty Moncur   \n",
       "\n",
       "              publishDate  \n",
       "0                09/14/08  \n",
       "1                09/28/04  \n",
       "2                05/23/06  \n",
       "3              10-10-2000  \n",
       "4              09-06-2006  \n",
       "...                   ...  \n",
       "52473       May 28th 2011  \n",
       "52474     August 5th 2011  \n",
       "52475     March 18th 2011  \n",
       "52476  September 1st 2011  \n",
       "52477        May 8th 2011  \n",
       "\n",
       "[52478 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = gr_data[['title','author','publishDate']]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15709b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54a708cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    title         author publishDate  isbn pages\n",
      "0  The Catcher in the Rye  J.D. Salinger        1951  None  None\n",
      "1   To Kill a Mockingbird     Harper Lee        1960  None  None\n",
      "2                    1984  George Orwell        1949  None  None\n",
      "3             Animal Farm  George Orwell        1945  None  None\n",
      "4     Pride and Prejudice    Jane Austen        1813  None  None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Set user-agent headers to avoid getting blocked\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "# Define a function to extract the ISBN and number of pages from a book's Goodreads page\n",
    "def extract_book_info(url):\n",
    "    # Send a request to the book's Goodreads page\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the ISBN and number of pages elements on the page\n",
    "    isbn_element = soup.find('span', {'itemprop': 'isbn'})\n",
    "    pages_element = soup.find('span', {'itemprop': 'numberOfPages'})\n",
    "\n",
    "    # Extract the text content of the elements\n",
    "    isbn = isbn_element.text if isbn_element else None\n",
    "    pages = int(pages_element.text.split()[0]) if pages_element else None\n",
    "\n",
    "    return isbn, pages\n",
    "\n",
    "# Create a new DataFrame to store the results\n",
    "result_df = subset[:10].copy()\n",
    "result_df['isbn'] = None\n",
    "result_df['pages'] = None\n",
    "\n",
    "# Iterate over the rows in the DataFrame and extract the ISBN and number of pages for each book\n",
    "for i, row in result_df.iterrows():\n",
    "    title = row['title']\n",
    "    author = row['author']\n",
    "    url = f'https://www.goodreads.com/search?q={title}+{author}&search_type=books&from=0&sort=stars&st=goodreads&utf8=%E2%9C%93'\n",
    "    time.sleep(1 + 2 * i)  # Sleep for increasing amounts of time to avoid getting blocked\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    results = soup.find_all('a', {'class': 'bookTitle'})\n",
    "    for result in results:\n",
    "        book_url = f'https://www.goodreads.com{result[\"href\"]}'\n",
    "        book_isbn, book_pages = extract_book_info(book_url)\n",
    "        if book_isbn is not None:\n",
    "            result_df.at[i, 'isbn'] = book_isbn\n",
    "            result_df.at[i, 'pages'] = book_pages\n",
    "            break  # Stop searching for ISBNs after the first successful match\n",
    "\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d600e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publishDate</th>\n",
       "      <th>isbn</th>\n",
       "      <th>pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Catcher in the Rye</td>\n",
       "      <td>J.D. Salinger</td>\n",
       "      <td>1951</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>1960</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1984</td>\n",
       "      <td>George Orwell</td>\n",
       "      <td>1949</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Animal Farm</td>\n",
       "      <td>George Orwell</td>\n",
       "      <td>1945</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>1813</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title         author publishDate  isbn pages\n",
       "0  The Catcher in the Rye  J.D. Salinger        1951  None  None\n",
       "1   To Kill a Mockingbird     Harper Lee        1960  None  None\n",
       "2                    1984  George Orwell        1949  None  None\n",
       "3             Animal Farm  George Orwell        1945  None  None\n",
       "4     Pride and Prejudice    Jane Austen        1813  None  None"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bee6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "9780316769174 (ISBN10: 0316769177)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0943e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f145c358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2249846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    title         author publishDate isbn pages\n",
      "0  The Catcher in the Rye  J.D. Salinger        1951           \n",
      "1   To Kill a Mockingbird     Harper Lee        1960           \n",
      "2                    1984  George Orwell        1949         12\n",
      "3             Animal Farm  George Orwell        1945           \n",
      "4     Pride and Prejudice    Jane Austen        1813           \n"
     ]
    }
   ],
   "source": [
    "import requests  # Import the requests library for making HTTP requests\n",
    "from bs4 import BeautifulSoup  # Import the BeautifulSoup library for parsing HTML\n",
    "import time  # Import the time library for sleeping\n",
    "import random  # Import the random library for generating random sleep durations\n",
    "\n",
    "# Assuming the subset dataframe has a column called 'title'\n",
    "subset['isbn'] = ''  # Add a new column called 'isbn' to the subset dataframe\n",
    "\n",
    "for i, title in enumerate(subset['title']):\n",
    "    # Construct the search URL for the current title by concatenating the title and the string 'isbn'\n",
    "    url = f'https://www.google.com/search?q={title}+isbn'\n",
    "\n",
    "    # Send a GET request to the search URL and store the response in a variable called 'response'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the response using BeautifulSoup and store the resulting object in a variable called 'soup'\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the first search result on the page by looking for the first div element with class 'g'\n",
    "    result = soup.find('div', {'class': 'g'})\n",
    "\n",
    "    if result:  # Check if the search result was found\n",
    "        # Find the ISBN number for the book by looking for the first div element with class 'sF6p5c' within the search result\n",
    "        isbn = result.find('div', {'class': 'sF6p5c'})\n",
    "\n",
    "        # Extract the ISBN number text from the div element and remove any leading or trailing whitespace\n",
    "        isbn_number = isbn.text.strip()\n",
    "\n",
    "        # Update the 'isbn' column of the current row in the subset dataframe with the extracted ISBN number\n",
    "        subset.at[i, 'isbn'] = isbn_number\n",
    "\n",
    "    # Generate a random sleep duration between 1 to 3 seconds using the random library\n",
    "    sleep_time = random.uniform(1, 3)\n",
    "\n",
    "    # Sleep for the generated duration using the time library\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "# Print the updated subset dataframe\n",
    "print(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "136fd74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publishDate</th>\n",
       "      <th>isbn</th>\n",
       "      <th>pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Catcher in the Rye</td>\n",
       "      <td>J.D. Salinger</td>\n",
       "      <td>1951</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>1960</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1984</td>\n",
       "      <td>George Orwell</td>\n",
       "      <td>1949</td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Animal Farm</td>\n",
       "      <td>George Orwell</td>\n",
       "      <td>1945</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>1813</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title         author publishDate isbn pages\n",
       "0  The Catcher in the Rye  J.D. Salinger        1951           \n",
       "1   To Kill a Mockingbird     Harper Lee        1960           \n",
       "2                    1984  George Orwell        1949         12\n",
       "3             Animal Farm  George Orwell        1945           \n",
       "4     Pride and Prejudice    Jane Austen        1813           "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72feea58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f89bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ca7ef71",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w7/7cfd14hn02g0d126w5q47rqw0000gn/T/ipykernel_20836/1840853560.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Find the ISBN number for the book by looking for the first div element with class 'sF6p5c' within the search result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0misbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'sF6p5c'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Extract the ISBN number text from the div element and remove any leading or trailing whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests  # Import the requests library for making HTTP requests\n",
    "from bs4 import BeautifulSoup  # Import the BeautifulSoup library for parsing HTML\n",
    "import time  # Import the time library for sleeping\n",
    "import random  # Import the random library for generating random sleep durations\n",
    "\n",
    "# Assuming the subset dataframe has a column called 'title'\n",
    "subset['isbn'] = ''  # Add a new column called 'isbn' to the subset dataframe\n",
    "\n",
    "for i, title in enumerate(subset['title']):\n",
    "    # Construct the search URL for the current title by concatenating the title and the string 'isbn'\n",
    "    url = f'https://www.google.com/search?q={title}+isbn'\n",
    "\n",
    "    # Send a GET request to the search URL and store the response in a variable called 'response'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the response using BeautifulSoup and store the resulting object in a variable called 'soup'\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the first search result on the page by looking for the first div element with class 'g'\n",
    "    result = soup.find('div', {'class': 'g'})\n",
    "\n",
    "    # Find the ISBN number for the book by looking for the first div element with class 'sF6p5c' within the search result\n",
    "    isbn = result.find('div', {'class': 'sF6p5c'})\n",
    "\n",
    "    # Extract the ISBN number text from the div element and remove any leading or trailing whitespace\n",
    "    isbn_number = isbn.text.strip()\n",
    "\n",
    "    # Update the 'isbn' column of the current row in the subset dataframe with the extracted ISBN number\n",
    "    subset.at[i, 'isbn'] = isbn_number\n",
    "\n",
    "    # Generate a random sleep duration between 1 to 3 seconds using the random library\n",
    "    sleep_time = random.uniform(1, 3)\n",
    "\n",
    "    # Sleep for the generated duration using the time library\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "# Print the updated subset dataframe\n",
    "print(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b6e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d72ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "761ff909",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w7/7cfd14hn02g0d126w5q47rqw0000gn/T/ipykernel_20836/3227257439.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Find the ISBN number for the book\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0misbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'a-text-normal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dir'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0misbn_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Assuming the subset dataframe has a column called 'title'\n",
    "subset['isbn'] = ''\n",
    "\n",
    "for i, title in enumerate(subset['title']):\n",
    "    url = f'https://www.amazon.com/s?k={title}&ref=nb_sb_noss'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the first result on the search page\n",
    "    result = soup.find('div', {'data-component-type': 's-search-result'})\n",
    "\n",
    "    # Find the ISBN number for the book\n",
    "    isbn = result.find('span', {'class': 'a-text-normal', 'dir': 'auto'})\n",
    "    isbn_number = isbn.text.strip()\n",
    "\n",
    "    subset.at[i, 'isbn'] = isbn_number\n",
    "\n",
    "    # Sleep for a random duration between 1 to 3 seconds\n",
    "    sleep_time = random.uniform(1, 3)\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "print(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7a52f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b65f5599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    title         author publishDate isbn pages\n",
      "0  The Catcher in the Rye  J.D. Salinger        1951           \n",
      "1   To Kill a Mockingbird     Harper Lee        1960           \n",
      "2                    1984  George Orwell        1949         12\n",
      "3             Animal Farm  George Orwell        1945           \n",
      "4     Pride and Prejudice    Jane Austen        1813           \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "\n",
    "# function to retrieve book information from Wikipedia API\n",
    "def get_book_info(title, author, publish_date):\n",
    "    # format the query string\n",
    "    query = f\"{title} {author} {publish_date} book\"\n",
    "    query = query.replace(' ', '+')\n",
    "\n",
    "    # make the API request\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch={query}&format=json\"\n",
    "    response = requests.get(url).json()\n",
    "\n",
    "    # check if there are search results\n",
    "    if 'search' in response['query']:\n",
    "        # get the title of the first search result\n",
    "        page_title = response['query']['search'][0]['title']\n",
    "\n",
    "        # retrieve the page content\n",
    "        url = f\"https://en.wikipedia.org/w/api.php?action=query&prop=revisions&rvprop=content&format=json&titles={page_title}\"\n",
    "        response = requests.get(url).json()\n",
    "\n",
    "        # parse the page content to extract the ISBN and page count\n",
    "        page_id = next(iter(response['query']['pages']))\n",
    "        page_content = response['query']['pages'][page_id]['revisions'][0]['*']\n",
    "        isbn = re.search(r'ISBN\\s+(?:\\d+\\-?)+', page_content)\n",
    "        pages = re.search(r'(\\d+)\\s+pages', page_content)\n",
    "\n",
    "        if isbn:\n",
    "            isbn = isbn.group(0)\n",
    "        else:\n",
    "            isbn = ''\n",
    "\n",
    "        if pages:\n",
    "            pages = pages.group(1)\n",
    "        else:\n",
    "            pages = ''\n",
    "\n",
    "        return isbn, pages\n",
    "\n",
    "    else:\n",
    "        return '', ''\n",
    "\n",
    "# create empty lists for the new columns\n",
    "subset['isbn'] = ''\n",
    "subset['pages'] = ''\n",
    "\n",
    "# iterate over the first 5 rows of the subset dataframe\n",
    "for index, row in subset[0:20].iterrows():\n",
    "    # get the book information\n",
    "    title = row['title']\n",
    "    author = row['author']\n",
    "    publish_date = row['publishDate']\n",
    "\n",
    "    isbn, pages = get_book_info(title, author, publish_date)\n",
    "\n",
    "    # add the information to the dataframe\n",
    "    subset.at[index, 'isbn'] = isbn\n",
    "    subset.at[index, 'pages'] = pages\n",
    "\n",
    "    # sleep for a random time between 1 to 3 seconds\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "\n",
    "# print the updated dataframe\n",
    "print(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba1c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec391312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb2d3f1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w7/7cfd14hn02g0d126w5q47rqw0000gn/T/ipykernel_20836/1505859499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mpages_selector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'#mw-content-text > div.mw-parser-output > table.infobox > tbody > tr:nth-child(11) > td > div > span'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0misbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misbn_selector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages_selector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "# select only the first 5 rows of the dataframe\n",
    "subset = subset.head()\n",
    "\n",
    "# iterate over each row in the subset\n",
    "for index, row in subset.iterrows():\n",
    "    # construct the search query based on the book's title and author\n",
    "    query = f\"{row['title']} by {row['author']}\"\n",
    "\n",
    "    # make a GET request to the Wikipedia page for the book\n",
    "    url = f\"https://en.wikipedia.org/wiki/{query}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # extract the ISBN and page count from the specific selectors\n",
    "    isbn_selector = '#mw-content-text > div.mw-parser-output > table.infobox > tbody > tr:nth-child(14) > td > div > span'\n",
    "    pages_selector = '#mw-content-text > div.mw-parser-output > table.infobox > tbody > tr:nth-child(11) > td > div > span'\n",
    "\n",
    "    isbn = soup.select_one(isbn_selector).text.strip()\n",
    "    pages = soup.select_one(pages_selector).text.strip()\n",
    "\n",
    "    # add the ISBN and page count to the dataframe as new columns\n",
    "    subset.at[index, 'ISBN'] = isbn\n",
    "    subset.at[index, 'page_count'] = pages\n",
    "\n",
    "    # sleep for a random amount of time between 1 and 3 seconds\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "\n",
    "# print the updated subset\n",
    "print(subset)\n",
    "\n",
    "# print the HTML content of the page\n",
    "print(soup.prettify())\n",
    "\n",
    "# print the selectors\n",
    "print(isbn_selector)\n",
    "print(pages_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dffc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b9f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0752d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "# iterate over each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    # construct the search query based on the book's title and author\n",
    "    query = f\"{row['title']} by {row['author']}\"\n",
    "\n",
    "    # make a GET request to the Wikipedia page for the book\n",
    "    url = f\"https://en.wikipedia.org/wiki/{query}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # extract the ISBN and page count from the specific selectors\n",
    "    isbn_selector = '#mw-content-text > div.mw-parser-output > table.infobox > tbody > tr:nth-child(14) > td > div > span'\n",
    "    pages_selector = '#mw-content-text > div.mw-parser-output > table.infobox > tbody > tr:nth-child(11) > td > div > span'\n",
    "\n",
    "    isbn = soup.select_one(isbn_selector).text.strip()\n",
    "    pages = soup.select_one(pages_selector).text.strip()\n",
    "\n",
    "    # add the ISBN and page count to the dataframe as new columns\n",
    "    df.at[index, 'ISBN'] = isbn\n",
    "    df.at[index, 'page_count'] = pages\n",
    "\n",
    "    # sleep for a random amount of time between 1 and 3 seconds\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "\n",
    "# print the updated dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850a2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000c772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359764af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91c296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7064017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_google(search_query):\n",
    "    url = f\"https://www.google.com/search?q={search_query}&tbm=bks\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    try:\n",
    "        isbn_selector = '#rso > div.ULSxyf > div > block-component > div > div.dG2XIf.XzTjhb > div > div > div > div > div.ifM9O > div > div > div > div > div.wDYxhc > div > div.webanswers-webanswers_table__webanswers-table > table > tbody > tr:nth-child(5) > td:nth-child(2)'\n",
    "        pages_selector = '#rso > div.ULSxyf > div > block-component > div > div.dG2XIf.XzTjhb > div > div > div > div > div.ifM9O > div > div > div > div > div.wDYxhc > div > div.webanswers-webanswers_table__webanswers-table > table > tbody > tr:nth-child(3) > td:nth-child(2)'\n",
    "        isbn = soup.select_one(isbn_selector).get_text()\n",
    "        pages = soup.select_one(pages_selector).get_text()\n",
    "    except AttributeError:\n",
    "        print(f\"Information not found for query: {search_query}\")\n",
    "        isbn = None\n",
    "        pages = None\n",
    "    return isbn, pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d90a7b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information not found for query: GoodReads The Catcher in the Rye by J.D. Salinger in 1951\n"
     ]
    }
   ],
   "source": [
    "isbn, pages = scrape_google('GoodReads The Catcher in the Rye by J.D. Salinger in 1951')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36df517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to scrape the Google search results page and extract the ISBN and pages information\n",
    "def scrape_google(search_query):\n",
    "    # Define the headers to be sent with the request\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "        'Accept-Language': 'en-US,en;q=0.8',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'DNT': '1',\n",
    "        'Connection': 'close',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "\n",
    "    # Send a request to the Google search results page with the search query and headers\n",
    "    url = f'https://www.google.com/search?q={search_query}'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract the ISBN and pages information from the relevant HTML tags and attributes\n",
    "    isbn_selector = '#rso > div.ULSxyf > div > block-component > div > div.dG2XIf.XzTjhb > div > div > div > div > div.ifM9O > div > div > div > div > div.wDYxhc > div > div.webanswers-webanswers_table__webanswers-table > table > tbody > tr:nth-child(5) > td:nth-child(2)'\n",
    "    pages_selector = '#rso > div.ULSxyf > div > block-component > div > div.dG2XIf.XzTjhb > div > div > div > div > div.ifM9O > div > div > div > div > div.wDYxhc > div > div.webanswers-webanswers_table__webanswers-table > table > tbody > tr:nth-child(3) > td:nth-child(2)'\n",
    "    isbn = soup.select_one(isbn_selector).get_text()\n",
    "    pages = soup.select_one(pages_selector).get_text()\n",
    "    \n",
    "    # Return the ISBN and pages information\n",
    "    return isbn, pages\n",
    "\n",
    "# Create a sample DataFrame\n",
    "subset = pd.DataFrame({'title': ['The Catcher in the Rye', 'To Kill a Mockingbird', '1984'],\n",
    "                       'author': ['J.D. Salinger', 'Harper Lee', 'George Orwell'],\n",
    "                       'publishDate': ['1951', '1960', '1949']})\n",
    "\n",
    "# Scrape the ISBN and pages information for each book in the DataFrame\n",
    "isbn_list = []\n",
    "pages_list = []\n",
    "for i in range(len(subset)):\n",
    "    search_query = f'GoodReads {subset.iloc[i][\"title\"]} by {subset.iloc[i][\"author\"]} in {subset.iloc[i][\"publishDate\"]}'\n",
    "    isbn, pages = scrape_google(search_query)\n",
    "    isbn_list.append(isbn)\n",
    "    pages_list.append(pages)\n",
    "    \n",
    "    # Add a random sleep between 1 and 3 seconds\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    # Print progress every 5000 rows\n",
    "    if (i+1) % 5000 == 0:\n",
    "        print(f'Scraped {i+1} rows...')\n",
    "\n",
    "# Add the ISBN and pages information to new columns in the DataFrame\n",
    "subset['isbn'] = isbn_list\n",
    "subset['pages'] = pages_list\n",
    "\n",
    "print(subset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
